{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing behavioural syllables using `keypoint-moseq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore a few simple analyses we can carry out with a trained `keypoint-moseq` model applied to new data.\n",
    "\n",
    "Applying a trained model to new data may be useful if you have collected data for new experiments but would like to maintain an existing set of syllables. \n",
    "\n",
    "We assume that you have gone through the `EPM_train_keypoint_moseq.ipynb` notebook, which explains how to create and train a `keypoint-moseq` model on a set of 10 videos. We also assume that you have downloaded the `mouse-EPM` folder with the sample data.\n",
    "\n",
    "This notebook is based on the one provided as part of the [`keypoint-moseq` documentation](https://github.com/dattalab/keypoint-moseq/blob/main/docs/keypoint_moseq_colab.ipynb).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Setup\n",
    "### A1. Create a conda environment and install the required packages\n",
    "We will use the conda environment from the `keypoint-moseq` documentation, with an optional additional package.\n",
    "\n",
    "In a terminal, clone the `keypoint-moseq` repository (ideally somewhere outside the `course-behavioural-analysis` repository):\n",
    "```bash\n",
    "git clone https://github.com/dattalab/keypoint-moseq\n",
    "cd keypoint-moseq\n",
    "```\n",
    "\n",
    "Then, create the appropriate conda environment for your platform. For example, for a Linux installation with a GPU we would run:\n",
    "```bash\n",
    "# Linux (GPU)\n",
    "conda env create -f conda_envs/environment.linux_gpu.yml\n",
    "``` \n",
    "For other platforms, please see the full list of commands in the [keypoint-moseq docs](https://keypoint-moseq.readthedocs.io/en/latest/install.html#install-using-conda).\n",
    "\n",
    "\n",
    "This last command will create a conda environment called `keypoint_moseq`. We can activate this environment by running:\n",
    "```bash\n",
    "conda activate keypoint-moseq\n",
    "```\n",
    "\n",
    "Optionally, to display interactive plots in the notebook, we can install the `ipympl` package in the `keypoint-moseq` environment:\n",
    "```bash\n",
    "pip install ipympl \n",
    "```\n",
    "\n",
    "Once all required packages are installed, you can re-open this notebook and select the `keypoint-moseq` kernel to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import keypoint_moseq as kpms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: to use interactive plots, install `ipympl` and run this cell\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Specify paths to trained model and new data\n",
    "\n",
    "You should modify the `DATA_DIR` path below to point to the directory where you downloaded the `mouse-EPM` folder containing the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.home() / \"Data\" / \"behav-analysis-course\" / \"mouse-EPM\"\n",
    "\n",
    "# path to new data \n",
    "filename = \"sub-01_ses-01_task-EPM_time-165049_video_DLC_resnet50_MouseTopDownSep13shuffle1_340000.h5\"\n",
    "filename_no_ext = Path(filename).stem\n",
    "file_path = DATA_DIR / \"derivatives\" / \"software-DLC_predictions\" / filename \n",
    "\n",
    "# path to kpt-moseq trained model\n",
    "project_dir = DATA_DIR / \"derivatives\" / \"software-kptmoseq_n-10_project\"\n",
    "model_name = '2024_09_19-15_54_42'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Apply trained `keypoint-moseq` model to new SLEAP predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the trained `keypoint-moseq` model and its corresponding `config` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model, data, metadata, current_iter = kpms.load_checkpoint(project_dir, model_name)\n",
    "\n",
    "# load config \n",
    "config = kpms.load_config(project_dir)  # noqa: E731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the keys and values in the config to inspect it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print config\n",
    "for k,v in config.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the new data to the `keypoint-moseq` model, we need to first parse it and format it as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse keypoint data\n",
    "coordinates, confidences, bodyparts = kpms.load_keypoints(str(file_path), \"deeplabcut\")\n",
    "\n",
    "# coordinates is a dictionary that for each DLC file, points\n",
    "# to an array of size (nframes, n_keypoints, n_spatial_dimensions),\n",
    "# that holds the position of every predicted keypoint, at every frame.\n",
    "print('------')\n",
    "print('Coordinates:')\n",
    "print(coordinates.keys())\n",
    "print(coordinates[filename_no_ext].shape)\n",
    "print('------')\n",
    "\n",
    "# confidences is a dictionary that for each DLC file, points\n",
    "# to an array of size (nframes, n_keypoints) that holds the\n",
    "# confidence values for each predicted keypoint.\n",
    "print('Confidences:')\n",
    "print(confidences.keys())\n",
    "print(confidences[filename_no_ext].shape)\n",
    "print('------')\n",
    "\n",
    "# bodyparts is a list of the string labels applied to the keypoints in the data\n",
    "print('Bodyparts')\n",
    "print(bodyparts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now express the parsed data in the format that the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data for model\n",
    "data, metadata = kpms.format_data(coordinates, confidences, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After formatting the new data, we can apply the trained model to it. \n",
    "\n",
    "By default, the results for the new data will be added to the existing `results.h5` file that lives under keypoint_moseq_project_dir / model_name.\n",
    "\n",
    "In our case, the file is at `mouse-EPM/derivatives/software-kptmoseq_n-10_project/2024_09_19-15_54_42/results.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trained model to new data;\n",
    "# the `apply_model` function extracts the results to a variable and \n",
    "# appends them to the existing results.h5 file.\n",
    "results = kpms.apply_model(\n",
    "    model, \n",
    "    data, \n",
    "    metadata, \n",
    "    project_dir, \n",
    "    model_name, \n",
    "    **config, \n",
    ")  \n",
    "\n",
    "# optionally, rerun `save_results_as_csv` to export the new results as a csv\n",
    "kpms.save_results_as_csv(results, project_dir, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `results` variable contains the results only for the new data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.keys())\n",
    "print(results[filename_no_ext].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load all the analysed data, including the data used to fit the model, we can use the `load_results` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = kpms.load_results(path=project_dir / model_name / \"results.h5\")\n",
    "\n",
    "# check data inside\n",
    "for k in results_all.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Compute frequency of syllables in the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect how many times each syllable appears in the new data.\n",
    "\n",
    "Remember that the syllables were computing using a larger dataset of keypoint predictions over 10 videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract array with the syllable assigned to every frame\n",
    "syllables_per_frame = results[filename_no_ext][\"syllable\"]  \n",
    "print(syllables_per_frame.shape) # size of array: (nframes, )\n",
    "\n",
    "# count the number of times each syllable appears in the video\n",
    "syllables_count = {}\n",
    "for syl in np.unique(syllables_per_frame):\n",
    "    syllables_count[syl] = sum(syl == syllables_per_frame)\n",
    "\n",
    "# sort syllables by count\n",
    "syllables_count = dict(\n",
    "    sorted(\n",
    "        syllables_count.items(), \n",
    "        key=lambda item: item[1], \n",
    "        reverse=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# print top 10 most frequent syllables\n",
    "# (the syllable IDs are assigned based on their frequency in the training data)\n",
    "n_frames = results[filename_no_ext][\"syllable\"].shape[0]\n",
    "for syl, count in list(syllables_count.items())[:10]:\n",
    "    print(f\"Syllable id-{syl}: {(count/n_frames)*100:.2f} % of frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Plot ethogram for new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute an ethogram of the new data using the predicted syllables.\n",
    "\n",
    "To do that, we first need to define all continuous sections of frames with that share a common syllable. \n",
    "\n",
    "Specifically, we compute the start frame and the duration of each continuous chunk of syllables using the `itertools.groupby()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find continuous chunks of frames with the same syllable\n",
    "# itertools.groupby: generates a break or new group\n",
    "# every time the value of the key function changes\n",
    "syllable_chunks = [\n",
    "    (key, len(list(group_iter)))\n",
    "    for key, group_iter in itertools.groupby(syllables_per_frame)\n",
    "]  # list of tuples, each tuple is a pair (syllable_id, lenght_of_chunk)\n",
    "\n",
    "chunks_duration = [syl_dur for syl_id, syl_dur in syllable_chunks]\n",
    "chunks_start = np.cumsum([0]+chunks_duration) - 0.5 # starting frame of each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can plot the data as an ethogram. For clarity, we focus on the first 1000 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare colormap for syllables\n",
    "list_colors = (\n",
    "    plt.get_cmap(\"tab10\").colors\n",
    "    + plt.get_cmap(\"tab20b\").colors\n",
    "    + plt.get_cmap(\"Set3\").colors\n",
    "    + plt.get_cmap(\"Set1\").colors\n",
    ")  # 51 colors\n",
    "\n",
    "# select maximum frames to plot\n",
    "frames_max_to_plot = 1000\n",
    "\n",
    "# plot ethogram as horizontal bar plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "rects = ax.barh(\n",
    "    y=results.keys(),\n",
    "    width=chunks_duration,\n",
    "    left=chunks_start[:-1],  \n",
    "    height=1,\n",
    "    color=[\n",
    "        list_colors[syl_id%len(list_colors)] \n",
    "        for syl_id, syl_dur in syllable_chunks\n",
    "    ],\n",
    ")\n",
    "ax.bar_label(\n",
    "    rects, \n",
    "    labels=[syl_id for syl_id, syl_dur in syllable_chunks],\n",
    "    label_type='center', \n",
    "    color='white'\n",
    ")\n",
    "ax.set_xlim(0, frames_max_to_plot)\n",
    "ax.set_xlabel('frames')\n",
    "ax.yaxis.set_visible(False)\n",
    "\n",
    "ax.set_aspect(100)\n",
    "ax.set_title('ethogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Compute median duration per syllable and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the median duration of all syllables in the new data.\n",
    "\n",
    "Keep in mind that the median syllable duration in the training data was ~16 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute median syllable duration\n",
    "median_syllable_duration = np.median(chunks_duration)\n",
    "print(f'The median syllable duration is {median_syllable_duration} frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compute the median duration per syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute median duration per syllable ID\n",
    "median_duration_per_syl = {}\n",
    "for syl in list(syllables_count.keys()):\n",
    "    median_duration_per_syl[syl] = np.median(\n",
    "        [\n",
    "            syl_dur \n",
    "            for (syl_id, syl_dur) in syllable_chunks \n",
    "            if syl_id == syl\n",
    "        ]\n",
    "    )  \n",
    "\n",
    "for k,v in median_duration_per_syl.items():\n",
    "    print(f\"The median duration for syllable-id {k} is {v} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot all the results together, we can use a simple scatter plot. The red reference line indicates the median duration across all syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot median duration per syllable\n",
    "fps = 30 \n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(\n",
    "    x=median_duration_per_syl.keys(),\n",
    "    y=median_duration_per_syl.values(),\n",
    ")\n",
    "ax.hlines(\n",
    "    y=median_syllable_duration,\n",
    "    xmin=-1,\n",
    "    xmax=len(median_duration_per_syl) + 1,\n",
    "    colors=\"r\",\n",
    ")\n",
    "ax.set_xlabel(\"syllable ID\")\n",
    "ax.set_ylabel(\"median duration (frames)\")\n",
    "\n",
    "print(f\"Median syllable duration (frames): {median_syllable_duration}\")\n",
    "print(f\"Median syllable duration (ms): {1000*median_syllable_duration/fps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Visualise the most frequent syllables in the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use the ethogram visualisation to get an intuition of how frequent specific syllables are.\n",
    "\n",
    "For each syllable, we plot the frames in which it is present in red, and the rest of frames in grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_max_to_plot = len(syllables_per_frame)\n",
    "\n",
    "for selected_syl in syllables_count.keys():\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    rects = ax.barh(\n",
    "        y=results.keys(),\n",
    "        width=[syl_dur for syl_id, syl_dur in syllable_chunks],\n",
    "        left=chunks_start[:-1],  # starting frame of each chunk\n",
    "        height=1,\n",
    "        color=[\n",
    "            'red' if syl_id==selected_syl \n",
    "            else 'grey' \n",
    "            for syl_id, _ in syllable_chunks\n",
    "        ],\n",
    "    )\n",
    "    # ax.bar_label(rects, label_type='center', color='white')\n",
    "    ax.set_xlim(0, frames_max_to_plot)\n",
    "    ax.set_xlabel('frames')\n",
    "    ax.yaxis.set_visible(False)\n",
    "\n",
    "    ax.set_aspect(int(frames_max_to_plot/10))\n",
    "    ax.set_title(f'Syllable {selected_syl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the syllables, only a few frames are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(syllables_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Plot centroid location for the top three syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During model fitting, `keypoint-moseq` computes the centroid of the animal as a latent variable.\n",
    "\n",
    "In this section, we plot the position of the centroid for the 3 most frequent syllables in the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_syllables = 3\n",
    "\n",
    "centroid_array = results[filename_no_ext]['centroid']\n",
    "syllable_array = results[filename_no_ext]['syllable']\n",
    "\n",
    "for syl_id in list(syllables_count.keys())[:top_k_syllables]:\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.scatter(\n",
    "        x=centroid_array[syllable_array == syl_id,0],\n",
    "        y=centroid_array[syllable_array == syl_id,1],\n",
    "        s=1,\n",
    "        color=list_colors[int(syl_id)%len(list_colors)]\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'Centroid - syllable {syl_id}')\n",
    "\n",
    "    # image size: 1298 × 1028\n",
    "    ax.set_xlim([0, 1298])\n",
    "    ax.set_ylim([0, 1028])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some syllables seem to occur more frequently on specific areas (for example, syllable 0 seems to be identified more often when the mouse is on the horizontal arm of the maze), whereas others seem more uniformly distributed (like syllable 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same data can be expressed as a heatmap. \n",
    "\n",
    "Below we do that by first computing a 2D discretised probability density function for each of the top-3 syllables.\n",
    "\n",
    "The values in the colorbar are proportional to the relative count of samples. Brighter colours indicate bins with a large number of samples relative to the total number of samples recorded for that syllable.\n",
    "\n",
    "The bins are 25 x 25 pixels in size.\n",
    "\n",
    "We assume the DLC keypoint data is expressed in an image cooordinate system with the origin at the centre of the top-left pixel.\n",
    "This is the case for [SLEAP](https://github.com/talmolab/sleap/discussions/1640#discussioncomment-7867277) and for [OpenCV](https://answers.opencv.org/question/35111/origin-pixel-in-the-image-coordinate-system-in-opencv/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_k_syllables = 3\n",
    "\n",
    "centroid_array = results[filename_no_ext]['centroid']\n",
    "syllable_array = results[filename_no_ext]['syllable']\n",
    "\n",
    "\n",
    "for syl_id in list(syllables_count.keys())[:top_k_syllables]:\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    # image size: 1298 × 1028\n",
    "    bin_width = 25 # in pixels\n",
    "    x_edges_in = np.arange(-0.5, 1298, bin_width)\n",
    "    y_edges_in = np.arange(-0.5, 1028, bin_width)\n",
    "\n",
    "    heatmap, xedges, yedges = np.histogram2d(\n",
    "        x=centroid_array[syllable_array == syl_id,0],\n",
    "        y=centroid_array[syllable_array == syl_id,1],\n",
    "        bins=[x_edges_in, y_edges_in], # xedges = yedges\n",
    "        density=True,\n",
    "    )\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "\n",
    "    im = ax.imshow(heatmap.T, extent=extent, origin='lower')\n",
    "\n",
    "    ax.set_title(f'Centroid heatmap - syllable {syl_id}')\n",
    "\n",
    "    \n",
    "    ax.set_xlim([0, 1298])\n",
    "    ax.set_ylim([0, 1028])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cb = fig.colorbar(im)\n",
    "    cb.set_label('density')  #  (bin count / total samples / bin area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results in the .h5 file follow the structure below. \n",
    "```\n",
    "    results.h5\n",
    "    ├──recording_name1\n",
    "    │  ├──syllable      # syllable labels (z)\n",
    "    │  ├──latent_state  # inferred low-dim pose state (x)\n",
    "    │  ├──centroid      # inferred centroid (v)\n",
    "    │  └──heading       # inferred heading (h)\n",
    "    ⋮\n",
    "```\n",
    "\n",
    "### Other useful links from the `keypoint-moseq` docs:\n",
    "\n",
    "- [An in-depth explanation of the modeling results](https://keypoint-moseq.readthedocs.io/en/latest/FAQs.html#interpreting-model-outputs).\n",
    "\n",
    "- More on [detecting existing syllables in new data](https://keypoint-moseq.readthedocs.io/en/latest/FAQs.html#detecting-existing-syllables-in-new-data).\n",
    "\n",
    "- More on [adding new data to the training set, and retraining (re-fitting) a model](https://keypoint-moseq.readthedocs.io/en/latest/FAQs.html#continue-model-fitting-but-with-new-data).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keypoint_moseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
