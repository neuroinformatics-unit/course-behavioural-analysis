---
title: Video-based analysis of animal behaviour
subtitle: SWC/GCNU Neuroinformatics Unit
author: Niko Sirmpilatze, Chang Huan Lo, Sof√≠a Mi√±ano
execute: 
  enabled: true
link-external-icon: true
format:
    revealjs:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        footer: "Sainsbury Wellcome Centre | 2024-10-03"
        slide-number: c
        menu:
            numbers: true
        chalkboard: true
        scrollable: false
        preview-links: false
        view-distance: 10
        mobile-view-distance: 10
        auto-animate: true
        auto-play-media: true
        code-overflow: wrap
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: Arial
          curve: linear
        title-slide-attributes: 
          data-background-color: "#000000"
          data-background-image: "img/swc-building.jpg" 
          data-background-size: "cover"
          data-background-position: "center"
          data-background-opacity: "0.6"
        aside-align: center
    html:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        date: "2024-10-03"
        toc: true
        code-overflow: scroll
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: Arial
          curve: linear
          margin-left: 0
        embed-resources: true
        page-layout: full
links:
  course-webpage: "https://software-skills.neuroinformatics.dev/courses/video-analysis.html"
  gh-repo: "https://github.com/neuroinformatics-unit/course-behavioural-analysis"
  these-slides: "https://neuroinformatics.dev/course-behavioural-analysis/#/title-slide"
  dropbox: "https://tinyurl.com/behav-analysis-course-data"
  menti: "https://www.menti.com/"
  menti-link: "https://www.menti.com/aldg47maopsr"
  menti-code: "`5804 2245`"
papers:
  neuro-needs-behav-title: "Neuroscience Needs Behavior: Correcting a Reductionist Bias"
  neuro-needs-behav-doi: "https://www.sciencedirect.com/science/article/pii/S0896627316310406"
  quant-behav-title: "Quantifying behavior to understand the brain"
  quant-behav-doi: "https://www.nature.com/articles/s41593-020-00734-z"
  open-source-title: "Open-source tools for behavioral video analysis: Setup, methods, and best practices"
  open-source-doi: "https://elifesciences.org/articles/79305"
---

# Introductions

[Neuroinformatics Unit (NIU)](https://neuroinformatics.dev/){style="text-align: center"}

:::: {.columns}
::: {.column width="33%"}
![](img/niko_sirmpilatze.png)
Niko Sirmpilatze
:::

::: {.column width="33%"}
![](img/chang_huan_lo.png)
Chang Huan Lo
:::

:::{.column width="33%"}
![](img/sofia_minano.png)
Sof√≠a Mi√±ano
:::
::::

## Course materials

| **Slides** | [neuroinformatics.dev/course-behavioural-analysis]({{< meta links.these-slides >}}) |
|----|------|
| Webpage | [software-skills.neurinformatics.dev/course-behavioural-analysis]({{< meta links.course-webpage >}}) |
| GitHub | [github.com/neuroinformatics-unit/course-behavioural-analysis]({{< meta links.gh-repo >}}) |
| Data* | [tinyurl.com/behav-analysis-course-data]({{< meta links.dropbox >}}) |

::: aside
*Data produced and shared by [*Loukia Katsouri, O'Keefe Lab*](https://www.sainsburywellcome.org/web/people/loukia-katsouri)
:::

## Schedule: morning {.smaller}

| Time | Topic | Goals |
|----|-------|--------|
| 10:00 - 10:20 | Welcome | Introductions, troubleshooting |
| 10:20 - 11:00 | Theory: Quantifying Behaviour | What is behaviour, detection & tracking, pose estimation |
| 11:00 - 12:00 | Practice: SLEAP I | Label data, train models |
| 12:00 - 12:15 | Coffee Break | |
| 12:15 - 13:00 | Practice: SLEAP II | Evaluate models, run inference |
| 13:00 - 14:00 | Lunch break | |

: {.striped}

## Schedule: afternoon {.smaller}

| Time | Topic | Goals |
|----|-------|--------|
| 14:00 - 15:30 | Practice: movement | Load pose tracks into Python, clean and visualise data, compute kinematics |
| 15:30 - 15:45 | Coffee break | |
| 15:45 - 16:30 | Theory: From behaviour to actions | Classifying behaviours, action segmentation |
| 16:30 - 17:30 | Practice: keypoint-moseq | Extract behavioural syllables |

: {.striped}

## Install software requirements {.smaller}

You were asked to pre-install two `conda` environments for the practical exercises. Check that you have them installed:

```{.bash code-line-numbers="false"}
$ conda env list
sleap
keypoint_moseq
```

::: {.fragment}
If you don't have them, you can create them as follows:

1. [**SLEAP**](https://sleap.ai/): Use the [conda package method](https://sleap.ai/installation.html#conda-package) from the SLEAP installation guide.
2. [**Keypoint-MoSeq**](https://keypoint-moseq.readthedocs.io): Use the recommended [conda installation method](https://keypoint-moseq.readthedocs.io/en/latest/install.html#install-using-conda).
:::

# Theory: Quantifying Behaviour {background-color="#03A062"}

## What is behaviour?

{{< include slides/go_to_menti.qmd >}}

## Defining behaviour {.smaller}

> The total movements made by the intact animal

::: {style="text-align: right"}
*Tinbergen, 1955*
:::

:::: {.fragment}

> Behavior is the **internally coordinated responses** (actions or inactions) of whole living organisms (individuals or groups) **to internal and/or external stimuli**, excluding responses more easily understood as developmental changes

::: {style="text-align: right"}
*Levitis et al., 2009*
:::

::::

::: aside
source: [{{< meta papers.neuro-needs-behav-title >}}]({{< meta papers.neuro-needs-behav-doi >}})
:::


## Why does neuroscience need behaviour?

{{< include slides/go_to_menti.qmd >}}


## Neuroscience needs behaviour {.smaller}

> ...detailed examination of brain parts or their selective perturbation is not sufficient to understand how the brain generates behavior


> ...it is very hard to infer the mapping between the behavior of a system and its lower-level properties by only looking at the lower-level properties


> The behavioral work needs to be as fine-grained as work at the neural level. Otherwise one is imperiled by a granularity mismatch between levels...

::: aside
source: [{{< meta papers.neuro-needs-behav-title >}}]({{< meta papers.neuro-needs-behav-doi >}})
:::


## Quantifying behaviour: ethogram {.smaller}

> **Ethogram:** a list of typical behaviours performed by an animal, including when and how often they occur

| Time after start (min) | Foraging | Eating | Grooming |
|------------------------|----------|--------|----------|
| 0:30                   | 0        | 0      | 1        |
| 1:00                   | 0        | 0      | 1        |
| 1:30                   | 1        | 0      | 0        |
| 2:00                   | 0        | 1      | 0        |

## Crab ethogram example {.smaller}

```{python}
import pandas as pd

crab_ethogram = pd.read_csv(
    "data/crab_ethogram_behaviours_Sanna.csv", usecols=[0, 1, 2]
)[:5]
crab_ethogram = crab_ethogram.style.set_properties(**{"text-align": "left"})
display(crab_ethogram)
```

::: aside
Credit to [*Sanna Titus, Branco & Margrie Labs*](https://www.sainsburywellcome.org/web/people/sanna-titus)
:::

## Crab ethogram data collection {visibility="uncounted" .smaller}

```{python}
crab_ethogram_data = pd.read_csv(
    "data/crab_ethogram_data_Sanna.csv", usecols=[0, 3, 4, 6, 7, 8, 9, 10, 11]
)[:4]
crab_ethogram_data = crab_ethogram_data.style.set_properties(**{"text-align": "left"})
display(crab_ethogram_data)
```

::: aside
Credit to [*Sanna Titus, Branco & Margrie Labs*](https://www.sainsburywellcome.org/web/people/sanna-titus)
:::

## Quantifying behaviour: modern {.smaller}

:::: {.columns}

::: {.column width="70%"}
![](img/modern_behav_experiment_analysis.png){fig-align="center" height="400px"}
:::

::: {.column width="30%"}
![](img/open-source-tools-fig2a.png){fig-align="center" height="400px"}
:::

::::

::: aside
source: [{{< meta papers.open-source-title >}}]({{< meta papers.open-source-doi >}})
:::


## Detection {.smaller}

:::: {.r-stack}

::: {layout-ncol=3 fig-align="center"}
![](img/mouse-frame.png)

![](img/mouse-mask.png)

![](img/mouse-centroid.png)
:::

::: {layout-ncol=3 fig-align="center"}
![](img/mouse-frame.png)

![](img/mouse-bbox.png){.fragment}

![](img/mouse-pose-estimation.png){.fragment}
:::

::::


## Tracking {.smaller}

:::: {.r-stack}

::: {layout-ncol=3 fig-align="center"}
![](img/mouse-centroid-tracking-single-animal.png)

![](img/mouse-bbox-tracking-single-animal.png)

![](img/mouse-pose-tracking-single-animal.png)
:::

::: {.fragment layout-ncol=3 fig-align="center"}
![](img/mouse-centroid-tracking-multi-animal.png)

![](img/mouse-bbox-tracking-multi-animal.png)

![](img/mouse-pose-tracking-multi-animal.png)
:::

::::

## Pose estimation {.smaller}

![](img/pose_estimation_2D.png){fig-align="center"}

- "easy" in humans - vast amounts of data
- "harder" in animals - less data, more variability


:::: aside
source: [{{< meta papers.quant-behav-title >}}]({{< meta papers.quant-behav-doi >}})
::::

## Pose estimation software {.smaller}

:::: {.columns}

:::{.column width="50%"}
[DeepLabCut](http://www.mackenziemathislab.org/deeplabcut): *transfer learning*
:::

::: {.column width="50%"}
[SLEAP](https://sleap.ai/):*smaller networks*
:::
::::

![source: [sleap.ai](https://sleap.ai/)](img/sleap_movie.gif){fig-align="center" height="350px" style="text-align: center"}

These handle pose estimation (detection) and tracking of single/multiple animals.

::: aside
Many others: 
[LightningPose](https://github.com/danbider/lightning-pose),
[DeepPoseKit](https://github.com/jgraving/DeepPoseKit),
[Anipose](https://anipose.readthedocs.io/en/latest/),
...
:::

## Multi-animal part grouping {.smaller}

::: {.r-stack}
![](img/mouse-multi-animal-keypoints.png)

![](img/mouse-part-grouping.png){.fragment}
:::

## Top-down vs bottom-up {.smaller}

![](img/pose_estimation_topdown.png){fig-align="center" height="230px"}

:::{.fragment}
![](img/pose_estimation_bottomup.png){fig-align="center" height="230px"}
:::

:::: aside
source: [{{< meta papers.quant-behav-title >}}]({{< meta papers.quant-behav-doi >}})
::::

## Multi-animal identity tracking {.smaller}

:::: {.columns}

::: {.column width="33%"}
![](img/mouse-identity-tracking.png)
:::

::: {.column width="33%" .fragment}
![](img/mouse-appearance-based-tracking.png)
:::

::: {.column width="33%" .fragment}
![](img/mouse-motion-based-tracking.png)
:::

::::

## 3D pose estimation {.smaller}

![](img/pose_estimation_3D.png){fig-align="center" height="400px"}

:::: aside
source: [{{< meta papers.quant-behav-title >}}]({{< meta papers.quant-behav-doi >}})
::::

# Practice: SLEAP I {background-color="#03A062"}

## Which mouse is more anxious? {.smaller}

[Click here to post your answers]({{< meta links.menti-link >}}){preview-link="true" style="text-align: center"}

:::: {.columns}

::: {.column width="50%"}
![sub-01](img/mouse1_EPM.gif){fig-align="center" height="400px" style="text-align: center"}
:::

::: {.column width="50%"}
![sub-02](img/mouse2_EPM.gif){fig-align="center" height="400px" style="text-align: center"}
:::

::::

## The Elevated Plus Maze {.smaller}

:::: {.columns}

::: {.column width="50%"}
![](img/mouse1_EPM.gif){fig-align="center" height="400px" style="text-align: center"}
:::

::: {.column width="50%"}
- **Structure:** 2 open arms, 2 closed arms, central area
- Exploits rodents' natural aversion to open spaces and height
- Less anxious animals spend more time in open arms
:::

::::

::: {.fragment style="text-align: center; color: #03A062;"}
**Task**: quantify time spent in open arms / closed arms
:::

## The dataset

`$ cd behav-analysis-course`
```{.bash code-line-numbers="false"}
behav-analysis-course/
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ mouse-EPM
    ‚îú‚îÄ‚îÄ derivatives
    ‚îî‚îÄ‚îÄ rawdata
```

::: {.fragment}
`$ cd mouse-EPM/rawdata`
```{.bash code-line-numbers="false"}
rawdata/
‚îú‚îÄ‚îÄ sub-01_ses-01_task-EPM_time-165049_video.mp4
‚îî‚îÄ‚îÄ sub-02_ses-01_task-EPM_time-185651_video.mp4
```
:::

::: aside
Roughly following the [NeuroBlueprint](https://neuroblueprint.neuroinformatics.dev/) specification.
:::

## The SLEAP workflow

![](img/pose-estimation.svg){fig-align="center" height="500px"}

## Create a new project

![](img/SLEAP_screenshots/1_add_videos.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Creating a project](https://sleap.ai/tutorials/new-project.html)
:::

## Define a skeleton {.smaller}

:::: {.columns}

::: {.column width="60%"}
![](img/mouse-annotated.png){fig-align="center" height="400px"}
:::

::: {.column width="40%"}
| Source | Destination |
|--------|-------------|
| snout  | left_ear    |
| snout  | right_ear   |
| snout  | centre      |
| left_ear  | centre   |
| right_ear  | centre  |
| centre | tail_base   |
| tail_base | tail_end |
:::
::::

:::{style="text-align: center; color: #03A062;"}
Save the project right after defining the skeleton!
:::

## Generate labeling suggestions
![](img/SLEAP_screenshots/5_generate_labelling_suggestions.png){fig-align="center" height="500px"}

## Label initial ~20 frames
![](img/SLEAP_screenshots/6_labelling.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Initial labeling](https://sleap.ai/tutorials/initial-labeling.html)
:::

## Start a training job 1/3
![](img/SLEAP_screenshots/7a_training_pipeline.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Configuring models](https://sleap.ai/guides/choosing-models.html)
:::

## Start a training job 2/3
![](img/SLEAP_screenshots/7b_training_centroid.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Configuring models](https://sleap.ai/guides/choosing-models.html)
:::

## Start a training job 3/3
![](img/SLEAP_screenshots/7c_training_centered_instance.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Configuring models](https://sleap.ai/guides/choosing-models.html)
:::

## Monitor training progress
![](img/SLEAP_screenshots/7d_training_progress.png){fig-align="center" height="500px"}


# Coffee break ‚òï {background-color="#1E1E1E"}

# Practice: SLEAP II {background-color="#03A062"}

## Evaluate trained models
![](img/SLEAP_screenshots/8_evaluation.png){fig-align="center" height="500px"}

::: aside
see also the SLEAP [model evaluation notebook](https://sleap.ai/notebooks/Model_evaluation.html)
:::

## Run inference on new frames
![](img/SLEAP_screenshots/9_inference.png){fig-align="center" height="400px"}

::: aside
- Expecte errors on silicon (M1/M2) Macs, see [this discussion](https://github.com/talmolab/sleap/discussions/1151)
- To correct predictions and update your training data, see SLEAP's  [Prediction-assisted labeling](https://sleap.ai/tutorials/assisted-labeling.html) and [Merging guide](https://sleap.ai/guides/merging.html).
:::

## Using SLEAP on the HPC cluster

- training and inference are GPU-intensive tasks
- SLEAP is installed as a module on SWC's HPC cluster
- `module load sleap`
- [See this guide for detailed instructions](https://howto.neuroinformatics.dev/data_analysis/HPC-module-SLEAP.html)
- [Come to the HPC course tomorrow](https://software-skills.neuroinformatics.dev/courses/hpc-behaviour.html)

## Predictions in the sample dataset {.smaller}

`$ cd behav-analysis-course/mouse-EPM/derivatives`
```{.bash code-line-numbers="false"}
derivatives/
‚îú‚îÄ‚îÄ software-DLC_predictions
‚îú‚îÄ‚îÄ software-SLEAP_project
    ‚îî‚îÄ‚îÄ predictions
```
::: {.fragment}
- Different pose estimation software produce predictions in different formats.
- Different workflows are needed for importing predicted poses into `Python` for further analysis.
  - e.g. for `SLEAP` see [Analysis examples](https://sleap.ai/notebooks/Analysis_examples.html)
:::

## What happens after pose tracking? {.smaller}

:::: {.columns}

::: {.column width="40%"}
![](img/open-source-tools-fig2a.png)
:::

::: {.column width="60%"}
- Load data into Python
- Visualise and inspect data
- Clean trajectories:
  - Identify and drop outliers
  - Smooth trajectories
  - Interpolate over missing data
- Compute variables of interest:
  - Velocity, acceleration, heading, etc.
  - Distances/angles between body parts
  - Time spent in different regions
  - Application-specific: navigation, social interactions, gait analysis
:::

::::

# Lunch break üçΩ {background-color="#1E1E1E"}

# Practice: movement {background-color="#03A062"}

## movement {.smaller}

A Python toolbox for analysing body movements across space and time, to aid the study of animal behaviour in neuroscience.

![](img/movement_overview.png){fig-align="center"}

::: aside
[GitHub repository](https://github.com/neuroinformatics-unit/movement) | [Documentation](https://movement.neuroinformatics.dev) | [Zulip chat](https://neuroinformatics.zulipchat.com/#narrow/stream/406001-Movement)
:::

## The movement poses dataset {.smaller}

:::: {.columns}
::: {.column width="25%"}
![](img/movement_poses_schematic.png){fig-align="center"}
:::

::: {.column width="75%" .fragment}
![](img/movement_poses_dataset.png){fig-align="center"}
:::
::::

::: aside
Powered by [`xarray`](https://docs.xarray.dev/en/latest/index.html) and its [data structures](https://tutorial.xarray.dev/fundamentals/01_datastructures.html)
:::

## The movement bboxes dataset {.smaller}

:::: {.columns}
::: {.column width="25%"}
![](img/movement_bboxes_schematic.png){fig-align="center"}
:::

::: {.column width="75%" .fragment}
![](img/movement_bboxes_dataset.png){fig-align="center"}
:::
::::

::: aside
Powered by [`xarray`](https://docs.xarray.dev/en/latest/index.html) and its [data structures](https://tutorial.xarray.dev/fundamentals/01_datastructures.html)
:::

## Time to play üõù with `movement` {.smaller}

In a terminal, clone [the course repository]({{< meta links.gh-repo >}}) and go to the notebooks directory:

```{.bash code-line-numbers="false"}
git clone https://github.com/neuroinformatics-unit/course-behavioural-analysis.git
cd course-behavioural-analysis/notebooks
```

::: {.fragment}
Create a new conda environment and install required packages:

```{.bash code-line-numbers="false"}
conda create -n epm-analysis -c conda-forge python=3.11 pip pytables
conda activate epm-analysis
pip install -r notebook_requirements.txt
```
:::

::: {.fragment}
Once all requirements are installed, you can:

- open the `EPM_analysis.ipynb` notebook
- select the environment `epm-analysis` as the kernel

We will go through the notebook step-by-step, together.
:::

## Which mouse was more anxious?
This time, with numbers!

{{< include slides/go_to_menti.qmd >}}

# Coffee break ‚òï {background-color="#1E1E1E"}

# Theory: From behaviour to actions {background-color="#03A062"}

## Discretising the continuous 1/2 {.smaller}

![](img/open-source-tools-fig1b.png){fig-align="center" height="400px"}

::: aside
source: [{{< meta papers.open-source-title >}}]({{< meta papers.open-source-doi >}})
:::

## Discretising the continuous 2/2 {.smaller}

![](img/behaviour-classification.png){fig-align="center" height=400px"}

::: aside
source: [{{< meta papers.quant-behav-title >}}]({{< meta papers.quant-behav-doi >}})
:::


## Supervised vs unsupervised tools { .smaller}
| **Supervised** | **Unsupervised** |
|----------------|------------------|
| [SimBA](https://github.com/sgoldenlab/simba) | [MotionMapper](https://github.com/gordonberman/MotionMapper) |
| [DeepEthogram](https://github.com/jbohnslav/deepethogram) | [MoSeq (depth- or keypoint-)](https://dattalab.github.io/moseq2-website/index.html) |
| [JAX Animal Behavior System](https://github.com/KumarLabJax/JABS-behavior-classifier) | [B-SOID](https://github.com/YttriLab/B-SOID) |
| [MARS](https://github.com/neuroethology/MARS) | [VAME](https://edspace.american.edu/openbehavior/project/vame/) |
| [DLC2action](https://github.com/amathislab/DLC2action/tree/master) |  |

: {.striped}

::: aside
Not a complete list!
:::


## Problems with supervised methods

{{< include slides/go_to_menti.qmd >}}

## Problems with unsupervised methods

{{< include slides/go_to_menti.qmd >}}

# Practice: keypoint-moseq {background-color="#03A062"}

## `keypoint-moseq` intro {.smaller}

Insert a brief description of the tool here.

## Time to play üõù with `keypoint-moseq`

::: {.incremental}
- Navigate to the same repository you cloned earlier `cd course-behavioural-analysis/notebooks`
- open the `EPM_syllables.ipynb` notebook
- select the environment `keypoint_moseq` as the kernel
:::

::: {.fragment}
We will go through the notebook step-by-step, together.
:::

# Feedback

Tell us what you think about this course!

Write on [IdeaBoardz](https://ideaboardz.com/for/course-behav-analysis-2023/5137372)
or talk to us anytime.

## Join the movement! {.smaller}

:::: {.columns}

::: {.column width="50%"}
![](img/movement_poses_schematic.png){fig-align="center" height="400px"}
:::

::: {.column width="50%"}
- Contributions to `movement` are absolutely encouraged, whether to fix a bug,
develop a new feature, improve the documentation, or just spark a discussion.

- [Chat with us on Zulip]((https://neuroinformatics.zulipchat.com/#narrow/stream/406001-Movement))

- Or [open an issue on GitHub](https://github.com/neuroinformatics-unit/movement/issues)
:::

::::

