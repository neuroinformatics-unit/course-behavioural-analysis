---
title: Video-based analysis of animal behaviour
subtitle: SWC/GCNU Neuroinformatics Unit
author: Niko Sirmpilatze, Chang Huan Lo, Sof√≠a Mi√±ano
execute: 
  enabled: true
link-external-icon: true
format:
    revealjs:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        footer: "Sainsbury Wellcome Centre | 2024-10-03"
        slide-number: c
        menu:
            numbers: true
        chalkboard: true
        scrollable: false
        preview-links: false
        view-distance: 10
        mobile-view-distance: 10
        auto-animate: true
        auto-play-media: true
        code-overflow: wrap
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: Arial
          curve: linear
        title-slide-attributes: 
          data-background-color: "#000000"
          data-background-image: "img/swc-building.jpg" 
          data-background-size: "cover"
          data-background-position: "center"
          data-background-opacity: "0.6"
        aside-align: center
    html:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        date: "2024-10-03"
        toc: true
        code-overflow: scroll
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: Arial
          curve: linear
          margin-left: 0
        embed-resources: true
        page-layout: full
links:
  course-webpage: "https://software-skills.neuroinformatics.dev/courses/video-analysis.html"
  gh-repo: "https://github.com/neuroinformatics-unit/course-behavioural-analysis"
  these-slides: "https://neuroinformatics.dev/course-behavioural-analysis/#/title-slide"
  dropbox: "https://tinyurl.com/behav-analysis-course-data"
  menti: "https://www.menti.com/"
  menti-link: "https://www.menti.com/aldg47maopsr"
  menti-code: "`5524 7145`"
papers:
  neuro-needs-behav-title: "Neuroscience Needs Behavior: Correcting a Reductionist Bias"
  neuro-needs-behav-doi: "https://www.sciencedirect.com/science/article/pii/S0896627316310406"
  quant-behav-title: "Quantifying behavior to understand the brain"
  quant-behav-doi: "https://www.nature.com/articles/s41593-020-00734-z"
  open-source-title: "Open-source tools for behavioral video analysis: Setup, methods, and best practices"
  open-source-doi: "https://elifesciences.org/articles/79305"
  moseq-title: "Mapping Sub-Second Structure in Mouse Behavior"
  moseq-doi: "https://doi.org/10.1016/j.neuron.2015.11.031"
  keypoint-moseq-title: "Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics"
  keypoint-moseq-doi: "https://doi.org/10.1038/s41592-024-02318-2"
---

# Introductions

[Neuroinformatics Unit (NIU)](https://neuroinformatics.dev/){style="text-align: center"}

:::: {.columns}
::: {.column width="33%"}
![](img/niko_sirmpilatze.png)
Niko Sirmpilatze
:::

::: {.column width="33%"}
![](img/chang_huan_lo.png)
Chang Huan Lo
:::

:::{.column width="33%"}
![](img/sofia_minano.png)
Sof√≠a Mi√±ano
:::
::::

## Course materials

| **Slides** | [neuroinformatics.dev/course-behavioural-analysis]({{< meta links.these-slides >}}) |
|----|------|
| Webpage | [software-skills.neurinformatics.dev/course-behavioural-analysis]({{< meta links.course-webpage >}}) |
| GitHub | [github.com/neuroinformatics-unit/course-behavioural-analysis]({{< meta links.gh-repo >}}) |
| Data* | [tinyurl.com/behav-analysis-course-data]({{< meta links.dropbox >}}) |

::: aside
*Data produced and shared by [*Loukia Katsouri, O'Keefe Lab*](https://www.sainsburywellcome.org/web/people/loukia-katsouri)
:::

## Schedule: morning {.smaller}

| Time | Topic | Goals |
|----|-------|--------|
| 10:00 - 10:20 | Welcome | Introductions, troubleshooting |
| 10:20 - 11:00 | Theory: Quantifying Behaviour | What is behaviour, detection & tracking, pose estimation |
| 11:00 - 12:00 | Practice: SLEAP I | Label data, train models |
| 12:00 - 12:15 | Coffee Break | |
| 12:15 - 13:00 | Practice: SLEAP II | Evaluate models, run inference |
| 13:00 - 14:00 | Lunch break | |

: {.striped}

## Schedule: afternoon {.smaller}

| Time | Topic | Goals |
|----|-------|--------|
| 14:00 - 15:45 | Practice: movement | Load pose tracks into Python, clean and visualise data, compute kinematics |
| 15:45 - 16:00 | Coffee break | |
| 16:00 - 16:30 | Theory: From behaviour to actions | Approaches to action segmentation |
| 16:15 - 17:30 | Demo: Keypoint-MoSeq | Extract behavioural syllables |

: {.striped}

## Install software requirements {.smaller}

You were asked to pre-install two `conda` environments for the practical exercises. Check that you have them installed:

```{.bash code-line-numbers="false"}
$ conda env list
sleap
keypoint_moseq
```

::: {.fragment}
If you don't have them, you can create them as follows:

1. [**SLEAP**](https://sleap.ai/): Use the [conda package method](https://sleap.ai/installation.html#conda-package) from the SLEAP installation guide.
2. [**Keypoint-MoSeq**](https://keypoint-moseq.readthedocs.io): Use the recommended [conda installation method](https://keypoint-moseq.readthedocs.io/en/latest/install.html#install-using-conda).
:::

{{< include slides/quantifying_behaviour.qmd >}}

{{< include slides/SLEAP.qmd >}}

# Lunch break üçΩ {background-color="#1E1E1E"}

{{< include slides/movement.qmd >}}

# Coffee break ‚òï {background-color="#1E1E1E"}

{{< include slides/from_behaviour_to_actions.qmd >}}

# Demo: Keypoint-MoSeq {background-color="#03A062"}

## Motion Sequencing {.smaller}

::: {layout="[[1,1,2]]"}

![Depth video recordings](img/depth-moseq.gif){fig-align="center" style="text-align: center" height="225px"}

![AR-HMM](img/depth-moseq-diagram.png){fig-align="center" style="text-align: center" height="225px"}

![depth-MoSeq](img/depth-moseq-syllables.png){fig-align="center" style="text-align: center" height="225px"}

:::

::: {.incremental}
- Timescale is controlled by the `kappa` parameter
- Higher `kappa` > higher P(self-transition) > "stickier" states > longer syllables
:::

::: aside
source: [{{< meta papers.moseq-title >}}]({{< meta papers.moseq-doi >}})
:::

## Keypoint-MoSeq {.smaller}

Can we apply MoSeq to keypoint data (predicted poses)?

![](img/depth-vs-keypoint-moseq.png){fig-align="center" height="350px"}

::: aside
source: [{{< meta papers.keypoint-moseq-title >}}]({{< meta papers.keypoint-moseq-doi >}})
:::

## Problems with keypoint data {.smaller}

::::: {.columns}

:::: {.column width="70%"}
![](img/keypoint-errors.png){width="600px"}

![](img/keypoint-jitter){width="610px"}
::::

:::: {.column width="30%"}

::: {.incremental}
- Keypoint noise leads to artifactual syllables
- We should somehow isolate true pose from noise
- But smoothing also blurs syllable boundaries
:::
::::

:::::

::: aside
source: [{{< meta papers.keypoint-moseq-title >}}]({{< meta papers.keypoint-moseq-doi >}})
:::


## Solution: a more complex model {.smaller}

**Switching Linear Dynamical System (SLDS):** combine noise-removal and action segmentation in a single probabilistic model

:::: {layout="[[1,1]]"}
![](img/moseq-model-diagrams.png)

::: {.r-stack}
![](img/allocentric-poses.png){.fragment}

![](img/egocentric-alignment.png){.fragment}

![](img/keypoint-moseq-modeling.png){.fragment}
:::

::::


## Keypoint-MoSeq drawbacks

::: {.incremental}
- probabilistic output
  - stochasticity of output syllables
  - must fit ensemble of models and take a "consensus"
- limited to describing behaviour at a single time-scale
  - but can be adapted by tuning `kappa`
- may miss rare behaviours (not often seen in training data)
:::

## Let's look at some syllables {.smaller}

We've trained a keypoint-MoSeq model on 10 videos from the (EPM) dataset.

```{.bash code-line-numbers="false"}
mouse-EPM/
‚îú‚îÄ‚îÄ derivatives
‚îÇ   ‚îî‚îÄ‚îÄ software-kptmoseq_n-10_project
‚îî‚îÄ‚îÄ rawdata
```

::: {.fragment}
![](img/all_trajectories.gif){fig-align="center" height="400px"}
:::

::: aside
The model was trained using the [EPM_train_keypoint_moseq.ipynb]({{< meta links.gh-repo >}}/blob/main/notebooks/EPM_train_keypoint_moseq.ipynb) notebook in the course's
[GitHub repository]({{< meta links.gh-repo >}}).
:::


## Time to play üõù with Keypoint-MoSeq

We will use the trained model to extract syllables from a new video.

::: {.fragment}
- Navigate to the same repository you cloned earlier `cd course-behavioural-analysis/notebooks`
- open the `EPM_syllables.ipynb` notebook
- select the environment `keypoint_moseq` as the kernel

We will go through the notebook step-by-step, together.
:::

# Feedback

Tell us what you think about this course!

Write on [IdeaBoardz](https://ideaboardz.com/for/course-behav-analysis-2024/5391792)
or talk to us anytime.

## Join the movement! {.smaller}

:::: {.columns}

::: {.column width="50%"}
![](img/movement_poses_schematic.png){fig-align="center" height="400px"}
:::

::: {.column width="50%"}
- Contributions to `movement` are absolutely encouraged, whether to fix a bug,
develop a new feature, improve the documentation, or just spark a discussion.

- [Chat with us on Zulip]((https://neuroinformatics.zulipchat.com/#narrow/stream/406001-Movement))

- Or [open an issue on GitHub](https://github.com/neuroinformatics-unit/movement/issues)
:::

::::

