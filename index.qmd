---
title: Video-based analysis of animal behaviour
subtitle: SWC/GCNU Neuroinformatics Unit
author: Niko Sirmpilatze, Chang Huan Lo, Alessandro Felder
execute: 
  enabled: true
link-external-icon: true
format:
    revealjs:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        footer: "Sainsbury Wellcome Centre | 2023-11-29"
        slide-number: c
        menu:
            numbers: true
        chalkboard: true
        scrollable: true
        preview-links: false
        view-distance: 10
        mobile-view-distance: 10
        auto-animate: true
        auto-play-media: true
        code-overflow: wrap
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: Arial
          curve: linear
        title-slide-attributes: 
          data-background-color: "#000000"
          data-background-image: "img/swc-building.jpg" 
          data-background-size: "cover"
          data-background-position: "center"
          data-background-opacity: "0.5"
        aside-align: center
    html:
        theme: [default, niu-light.scss]
        logo: img/logo_niu_light.png
        date: "2023-11-29"
        toc: true
        code-overflow: scroll
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: Arial
          curve: linear
          margin-left: 0
        embed-resources: true
        page-layout: full
links:
  course-webpage: "https://software-skills.neuroinformatics.dev/courses/video-analysis.html"
  gh-repo: "https://github.com/neuroinformatics-unit/course-behavioural-analysis-2023"
  these-slides: "https://neuroinformatics.dev/course-behavioural-analysis-2023/#/title-slide"
  dropbox: "https://www.dropbox.com/scl/fo/ey7b6yrqax2olqyv1th7j/h?rlkey=u4wh2gxtbbn4g5o3s55zbx6pp&dl=0"
  menti-link: "https://www.menti.com/aldg47maopsr"
  menti-code: "`5306 9550`"
papers:
  neuro-needs-behav-title: "Neuroscience Needs Behavior: Correcting a Reductionist Bias"
  neuro-needs-behav-doi: "10.1016/j.neuron.2016.12.041"
  quant-behav-title: "Quantifying behavior to understand the brain"
  quant-behav-doi: "10.1038/s41593-020-00734-z"
  open-source-title: "Open-source tools for behavioral video analysis: Setup, methods, and best practices"
  open-source-doi: "10.7554/eLife.79305"
---

## Introductions

[Neuroinformatics Unit (NIU)](https://neuroinformatics.dev/){preview-link="true" style="text-align: center"}

:::: {.columns}
::: {.column width="33%"}
![](img/niko_sirmpilatze.png)
Niko Sirmpilatze
:::

::: {.column width="33%"}
![](img/chang_huan_lo.png)
Chang Huan Lo
:::

:::{.column width="33%"}
![](img/alessandro_felder.png)
Alessandro Felder
:::
::::

## Course materials {.smaller}

#### These slides
- [neuroinformatics.dev/course-behavioural-analysis-2023]({{< meta links.these-slides >}})

#### Course webpage
- [software-skills.neurinformatics.dev/course-behavioural-analysis-2023]({{< meta links.course-webpage >}})

#### GitHub repository
- [github.com/neuroinformatics-unit/course-behavioural-analysis-2023]({{< meta links.gh-repo >}})

#### Sample data
- [Dropbox link]({{< meta links.dropbox >}}) OR...
- `/ceph/scratch/neuroinformatics-dropoff/behav-analysis-course`
- credits to [*Loukia Katsouri, O'Keefe Lab*](https://www.sainsburywellcome.org/web/people/loukia-katsouri)

## Schedule morning {.smaller}

**10:00 - 10:15: Welcome and troubleshooting**

**10:15 - 11:00: Theory I**

- What is behaviour and why do we study it?
- Tracking animals with pose estimation

**11:00 - 12:00: Practical I**

- Annotating video frames
- Training a pose estimation model

**12:00 - 13:30: Lunch break and SWC lab meeting**

## Schedule afternoon {.smaller}

**13:30 - 15:00: Practical II**

- Evaluating trained models
- Running inference on new videos
- Importing predictions into `Python`
- Plotting and analysing predictions

**15:00 - 15:20: Break**

**15:20 - 16:00: Theory II**

- Classifying and segmenting behaviours

**16:00 - 17:00: Practical III**

- Behaviour classification with MoSeq

## Install SLEAP via conda {.smaller}

Read the official [SLEAP installation guide](https://sleap.ai/installation.html).
If you already have `conda` installed, you may skip the `mamba` installation steps outlined there.
Instead, install the `libmamba-solver` for `conda`:

```{.bash}
conda install -n base conda-libmamba-solver
conda config --set solver libmamba
```

:::: {.fragment}
After that, you can follow the [rest of the SLEAP installation guide](https://sleap.ai/installation.html#conda-package), substituting `conda` for `mamba` in the relevant commands.

::: {.panel-tabset}

### Windows & Linux
```{.bash}
conda create -y -n sleap -c conda-forge -c nvidia -c sleap -c anaconda sleap=1.3.1
```

### MacOS
```{.bash}
conda create -y -n sleap -c conda-forge -c anaconda -c sleap sleap=1.3.1
```
:::

::::

## What is behaviour?

{{< include slides/go_to_menti.qmd >}}

## Defining behaviour {.smaller}

> The total movements made by the intact animal

::: {style="text-align: right"}
*Tinbergen, 1955*
:::

:::: {.fragment}

> Behavior is the **internally coordinated responses** (actions or inactions) of whole living organisms (individuals or groups) **to internal and/or external stimuli**, excluding responses more easily understood as developmental changes

::: {style="text-align: right"}
*Levitis et al., 2009*
:::

::::

::: aside
source: [{{< meta papers.neuro-needs-behav-title >}}]({{< meta papers.neuro-needs-behav-doi >}})
:::

## Neural activity and behaviour {.smaller}
![](img/neuro_behav_mappings.png){fig-align="center" height="500px"}

::: aside
source: [{{< meta papers.neuro-needs-behav-title >}}]({{< meta papers.neuro-needs-behav-doi >}})
:::

## Marr's three levels of analysis {.smaller}

![](img/marr_three_levels.png){fig-align="center" height="500px"}

::: aside
source: [{{< meta papers.neuro-needs-behav-title >}}]({{< meta papers.neuro-needs-behav-doi >}})
:::

## Neuroscience needs behaviour {.smaller}

::: {.fragment}
> ...detailed examination of brain parts or their selective perturbation is not sufficient to understand how the brain generates behavior
:::

::: {.fragment}
> ...it is very hard to infer the mapping between the behavior of a system and its lower-level properties by only looking at the lower-level properties
:::

::: {.fragment}
> The behavioral work needs to be as fine-grained as work at the neural level. Otherwise one is imperiled by a granularity mismatch between levels...
:::

::: {.fragment}
> ...the explanations of the results at the neural level are almost entirely dependent on the higher-level vocabulary and concepts derived from behavioral work. Lower levels of explanation do not "explain away" higher levels.
:::

::: {style="text-align: right"}
*Krakauer et al., 2017*
:::


## Quantifying behaviour: ethogram {.smaller}

> **Ethogram:** a list of typical behaviours performed by an animal, including when and how often they occur

| Time after start (min) | Foraging | Eating | Grooming |
|------------------------|----------|--------|----------|
| 0:30                   | 0        | 0      | 1        |
| 1:00                   | 0        | 0      | 1        |
| 1:30                   | 1        | 0      | 0        |
| 2:00                   | 0        | 1      | 0        |

## Quantifying behaviour: modern {.smaller}

![](img/modern_behav_experiment_analysis.png){fig-align="center" height="500px"}

::: aside
source: [{{< meta papers.open-source-title >}}]({{< meta papers.open-source-doi >}})
:::


## Finding and tracking animals {.smaller}

::: {layout-ncol=4 fig-align="center" height="180px"}
![](img/mouse-frame.png)

![](img/mouse-centroid.png)

![](img/mouse-ellipse.png)

![](img/mouse-pose-estimation.png)
:::

::: {.fragment layout-ncol=4 fig-align="center" height="180px"}
![](img/mouse-centroid-tracking.png)

![](img/mouse-ellipse-tracking.png)

![](img/mouse-pose-tracking-single-animal.png)

![](img/mouse-pose-tracking-multi-animal.png)
:::

## Pose estimation {.smaller}

![](img/pose_estimation_2D.png){fig-align="center"}

- "easy" in humans - vast amounts of data
- "harder" in animals - less data, more variability

:::: aside
source: [{{< meta papers.quant-behav-title >}}]({{< meta papers.quant-behav-doi >}})
::::

## Pose estimation software {.smaller}

:::: {.columns}

:::{.column width="50%"}
[DeepLabCut](http://www.mackenziemathislab.org/deeplabcut): *transfer learning*
:::

::: {.column width="50%"}
[SLEAP](https://sleap.ai/):*smaller networks*
:::
::::

![source: [sleap.ai](https://sleap.ai/)](img/sleap_movie.gif){fig-align="center" height="400px" style="text-align: center"}

::: aside
Many others: 
[LightningPose](https://github.com/danbider/lightning-pose),
[DeepPoseKit](https://github.com/jgraving/DeepPoseKit),
[Anipose](https://anipose.readthedocs.io/en/latest/),
...
:::

## Multi-animal part grouping {.smaller}

:::: {.columns}

::: {.column width="50%"}
![](img/mouse-multi-animal-keypoints.png)
:::

::: {.column width="50%" .fragment}
![](img/mouse-part-grouping.png)
:::

::::

## Top-down vs bottom-up {.smaller}

![](img/pose_estimation_topdown.png){fig-align="center" height="230px"}

:::{.fragment}
![](img/pose_estimation_bottomup.png){fig-align="center" height="230px"}
:::

:::: aside
source: [{{< meta papers.quant-behav-title >}}]({{< meta papers.quant-behav-doi >}})
::::

## Multi-animal identity tracking {.smaller}

:::: {.columns}

::: {.column width="33%"}
![](img/mouse-identity-tracking.png)
:::

::: {.column width="33%" .fragment}
![](img/mouse-appearance-based-tracking.png)
:::

::: {.column width="33%" .fragment}
![](img/mouse-motion-based-tracking.png)
:::

::::

## 3D pose estimation {.smaller}

![](img/pose_estimation_3D.png){fig-align="center" height="400px"}

:::: aside
source: [{{< meta papers.quant-behav-title >}}]({{< meta papers.quant-behav-doi >}})
::::

## Which mouse is more anxious? {.smaller}

[Click here to post your answers]({{< meta links.menti-link >}}){preview-link="true" style="text-align: center"}

:::: {.columns}

::: {.column width="50%"}
![sub-01](img/mouse1_EPM.gif){fig-align="center" height="400px" style="text-align: center"}
:::

::: {.column width="50%"}
![sub-02](img/mouse2_EPM.gif){fig-align="center" height="400px" style="text-align: center"}
:::

::::

## The Elevated Plus Maze {.smaller}

:::: {.columns}

::: {.column width="50%"}
![](img/mouse1_EPM.gif){fig-align="center" height="400px" style="text-align: center"}
:::

::: {.column width="50%"}
- **Structure:** 2 open arms, 2 closed arms, central area
- Exploits rodents' natural aversion to open spaces and height
- Less anxious animals spend more time in open arms
:::

::::

::: {.fragment style="text-align: center; color: #03A062;"}
**Task**: quantify % time spent in open arms
:::

## The dataset {.smaller}

`$ cd behav-analysis-course`
```{.bash}
.
├── LICENSE
├── README.md
└── mouse-EPM
    ├── derivatives
    └── rawdata
```

::: {.fragment}
`$ cd mouse-EPM/rawdata`
```{.bash}
.
├── sub-01_id-M708149
│   └── ses-01_date-20200317
│       └── behav
│           └── sub-01_ses-01_task-EPM_time-165049_video.mp4
└── sub-02_id-M708154
    └── ses-01_date-20200317
        └── behav
            └── sub-02_ses-01_task-EPM_time-185651_video.mp4
```
:::

::: aside
Organised according to the [NeuroBlueprint](https://neuroblueprint.neuroinformatics.dev/) specification.
:::

## The SLEAP workflow

![](img/pose-estimation.svg){fig-align="center" height="500px"}

## Create a new project

![](img/SLEAP_screenshots/1_add_videos.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Creating a project](https://sleap.ai/tutorials/new-project.html)
:::

## Define a skeleton {.smaller}

:::: {.columns}

::: {.column width="60%"}
![](img/mouse-annotated.png){fig-align="center" height="400px"}
:::

::: {.column width="40%"}
| Source | Destination |
|--------|-------------|
| snout  | left_ear    |
| snout  | right_ear   |
| snout  | centre      |
| left_ear  | centre   |
| right_ear  | centre  |
| centre | tail_base   |
| tail_base | tail_end |
:::
::::

:::{style="text-align: center; color: #03A062;"}
Save the project right after defining the skeleton!
:::

## Generate labeling suggestions
![](img/SLEAP_screenshots/5_generate_labelling_suggestions.png){fig-align="center" height="500px"}

## Label initial ~20 frames
![](img/SLEAP_screenshots/6_labelling.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Initial labeling](https://sleap.ai/tutorials/initial-labeling.html)
:::

## Start a training job 1/3
![](img/SLEAP_screenshots/7a_training_pipeline.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Configuring models](https://sleap.ai/guides/choosing-models.html)
:::

## Start a training job 2/3
![](img/SLEAP_screenshots/7b_training_centroid.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Configuring models](https://sleap.ai/guides/choosing-models.html)
:::

## Start a training job 3/3
![](img/SLEAP_screenshots/7c_training_centered_instance.png){fig-align="center" height="500px"}

::: aside
see SLEAP's [Configuring models](https://sleap.ai/guides/choosing-models.html)
:::

## Monitor training progress
![](img/SLEAP_screenshots/7d_training_progress.png){fig-align="center" height="500px"}

## Evaluate trained models
![](img/SLEAP_screenshots/8_evaluation.png){fig-align="center" height="500px"}

::: aside
see also the SLEAP [model evaluation notebook](https://sleap.ai/notebooks/Model_evaluation.html){preview-link="true"}
:::

## Run inference on new frames
![](img/SLEAP_screenshots/9_inference.png){fig-align="center" height="450px"}

::: aside
To correct predictions and update your training data, see SLEAP's  [Prediction-assisted labeling](https://sleap.ai/tutorials/assisted-labeling.html) and [Merging guide](https://sleap.ai/guides/merging.html).
:::

## Enter `movement`  {.smaller}

:::: {.columns}

::: {.column width="55%"}
![](img/movement-repo-screenshot.png)
:::

::: {.column width="45%"}
Python tools for analysing body movements across space and time.

- [GitHub repository](https://github.com/neuroinformatics-unit/movement)
- [Documentation](https://movement.neuroinformatics.dev)
- [PyPI package](https://pypi.org/project/movement/)
- [Zulip chat](https://neuroinformatics.zulipchat.com/#narrow/stream/406001-Movement)
:::

::::

